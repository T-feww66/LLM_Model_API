import pandas as pd
from selenium.webdriver.chrome.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException, ElementClickInterceptedException, WebDriverException
from time import sleep
import random

from utils.login import FacebookLogin
from scrapers.crawl_comment import CrawlComment


class CrawlPost:
    def __init__(self, driver: WebDriver, cookies_file: str, word_search: str):
        # Kh·ªüi t·∫°o c√°c thu·ªôc t√≠nh c·∫ßn thi·∫øt
        self.driver = driver
        self.cookies_file = cookies_file
        self.word_search = word_search


        self.xpath_button_comment = "//div[@role='button'and @id]"
        self.button_close = "//div[@aria-label='ƒê√≥ng'and @role = 'button']"
        self.posts_element = "//div[@aria-posinset and @aria-describedby]"

    # def clean_data(self, df):
    #     df = df.drop_duplicates(subset="post_id")
    #     return df
    

    def crawl_comment_fanpages_by_post(self, fanpages_file: str):
        df = pd.read_csv(fanpages_file)
        group_urls = df["fanpage_url"].tolist()
        comments = []

        for i, url in enumerate(group_urls):
            isLogin = FacebookLogin(driver=self.driver, cookie_path=self.cookies_file).login_with_cookies()
            if not isLogin:
                print(f"‚ùå Kh√¥ng th·ªÉ ƒëƒÉng nh·∫≠p fanpage {url}")
                continue

            print("‚úÖ V√†o fanpage:", i)
            self.driver.get(url)
            sleep(random.uniform(3, 5))

            for scroll_time in range(2):
                print(f"üîÑ Cu·ªôn trang load b√†i vi·∫øt l·∫ßn {scroll_time}")
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                sleep(random.uniform(5, 10))
                
                try:
                    link_element = WebDriverWait(self.driver, 15).until(
                        EC.presence_of_all_elements_located((By.XPATH, self.xpath_button_comment))
                    )

                    for idx, link in enumerate(link_element):
                        self.driver.execute_script("arguments[0].scrollIntoView({block: 'end'});", link)
                        sleep(random.uniform(1, 3))
                        self.driver.execute_script("arguments[0].click();", link)
                        print("ƒë√£ click v√†o: ", link.text)

                        sleep(random.uniform(2, 5))
                        print("B·∫Øt ƒë·∫ßu crawl comments")
                        comment_data = CrawlComment(driver=self.driver, cookies_file=self.cookies_file).crawl_comment_fanpage(self.word_search)

                        print(f"‚úÖ L·∫•y xong b√†i post th·ª©: {idx}")
                        if comment_data:
                            comments.extend(comment_data)
                except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException) as e:
                    print(f"‚ùå B√†i vi·∫øt {idx} kh√¥ng c√≥ b√¨nh lu·∫≠n ho·∫∑c l·ªói")
                    continue
                except Exception as e:
                    print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω b√†i post {idx}")
                    continue
        return pd.DataFrame(comments)
    def run(self):
        """ Ch·∫°y t·∫•t c·∫£ c√°c qu√° tr√¨nh """
        self.crawl_post_id()
        self.driver.quit()